<!DOCTYPE html>
<html>

<head>
  <meta charset="utf-8">
  <meta name="description"
    content="Safe-CLIP: Removing NSFW Concepts from Vision-and-Language Models">
  <meta name="keywords" content="Safe-CLIP, Trustworthy, Contrastive Learning">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>Safe-CLIP: Removing NSFW Concepts from Vision-and-Language Models
  </title>

  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro" rel="stylesheet">

  <link rel="stylesheet" href="./static/css/bulma.min.css">
  <link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="./static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="./static/css/index.css">
  <link rel="icon" href="static/images/unimore_logo.png">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="./static/js/fontawesome.all.min.js"></script>
  <script src="./static/js/bulma-carousel.min.js"></script>
  <script src="./static/js/bulma-slider.min.js"></script>
  <script src="./static/js/index.js"></script>
  <style>
    .carousel {
      display: flex;
      overflow: hidden;
      position: relative;
      width: 80%; 
      height: 50%;
      /* center this carousel */
      margin: 0 auto;
    }

    .carousel .item {
      flex: 0 0 25%; /* Display 4 items at once */
      transition: transform 0.5s ease;
      overflow: hidden;
      display: contents;
      align-items: center;
      justify-content: center;
      height: 100%; /* Ensure items take the full height of the carousel */
    }
    
    .carousel img {
      max-width: 100%;
      max-height: 100%;
      height: 100%; /* Adjust the height to match the container */
      width: auto;
      object-fit: contain; /* Ensure the entire image is visible within the container */
    }
    
    .carousel-buttons {
      position: absolute;
      top: 50%;
      width: 100%;
      display: flex;
      justify-content: space-between;
      transform: translateY(-50%);
    }
    
    .carousel-buttons button {
      background: rgba(255, 255, 255, 0.8);
      border: none;
      font-size: 2rem;
      cursor: pointer;
    }

    .slider .slider-navigation-next {
      display: none;
    }

    .slider .slider-navigation-previous {
      display: none;
    }

    </style>
</head>


<section class="hero">
  <div class="hero-body">
    <div class="container is-max-desktop">
      <div class="columns is-centered">
        <div class="column has-text-centered">
          <h1 class="title is-1 publication-title">Safe-CLIP: Removing NSFW Concepts from Vision-and-Language Models</h1>
          <div class="is-size-5 publication-authors">
            <h1 class="title is-4" style="color: #5c5c5c;">ECCV 2024</h1>

            <span class="author-block">
              <a href="https://...">Samuele Poppi</a>*<sup>1,2</sup>,
            <span class="author-block">
              <a href="https://...">Tobia Poppi</a>*<sup>1,2</sup>,
            </span>
            <span class="author-block">
                <a href="https://federico1-creator.github.io/Federico_Cocchi/">Federico Cocchi</a>*<sup>1,2</sup>,
            </span>
            <br>
            <span class="author-block">
              <a href="https://...">Marcella Cornia</a><sup>1</sup>,
            </span>
            <span class="author-block">
              <a href="https://...">Lorenzo Baraldi</a><sup>1</sup>,
            </span>
            <span class="author-block">
              <a href="https://...">Rita Cucchiara</a><sup>1,3</sup>
            </span>
          </div>

          <div class="is-size-5 publication-authors">
            <span class="author-block"><sup>1</sup>University of Modena and Reggio Emilia,</span>
            <span class="author-block"><sup>2</sup>University of Pisa,</span>
            <span class="author-block"><sup>3</sup>IIT-CNR, Italy</span>
          </div>
          <div class="is-size-5 publication-authors">
            <span class="author-block">* Equal contribution</span>
          </div>

          <div class="column has-text-centered">

            <span class="link-block">
              <a href="https://github.com/aimagelab/safe-clip"
                 class="external-link button is-normal is-rounded is-dark">
                <span class="icon">
                    <i class="fab fa-github"></i>
                </span>
                <span>Code</span>
                </a>
            </span>

            <span class="link-block">
              <a href="https://arxiv.org/abs/2311.16254" class="external-link button is-normal is-rounded is-dark">
                <span class="icon">
                  <i class="ai ai-arxiv"></i>
                </span>
                <span>arXiv</span>
              </a>
            </span>

              <!-- <span class="link-block">
                <a href="https://..."
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <svg class="svg-inline--fa fa-face-smiling-hands" aria-hidden="true" focusable="false" data-prefix="fas" data-icon="face-smiling-hands" role="img" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 640 512" data-fa-i2svg=""><path fill="currentColor" d="M411.1 495.3C382.8 506.1 352.1 512 319.1 512C287.9 512 257.2 506.1 228.9 495.3C245.9 473.7 255.1 446.4 255.1 416.8V386C274.1 394.4 295.4 400 319.1 400C344.6 400 365.9 394.4 384 386V416.8C384 446.4 394.1 473.7 411.1 495.3V495.3zM575.7 242.6C558.8 236.8 539.5 240.6 526.1 254.1L478.1 301.1C469.6 287.4 453.9 278.4 436 278.4C407.3 278.4 384 301.7 384 330.4V349.6C367.2 360.3 345.9 368 319.1 368C294.1 368 272.8 360.3 255.1 349.6V330.4C255.1 301.7 232.7 278.4 203.1 278.4C186.1 278.4 170.4 287.4 161 301.1L113.9 254.1C100.5 240.6 81.15 236.8 64.34 242.6C71.31 107.5 183.1 0 319.1 0C456.9 0 568.7 107.5 575.7 242.6V242.6zM281.6 228.8C283.7 231.6 287.3 232.7 290.5 231.6C293.8 230.5 295.1 227.4 295.1 224C295.1 206.1 289.3 188.4 279.4 175.2C269.6 162.2 255.5 152 239.1 152C224.5 152 210.4 162.2 200.6 175.2C190.7 188.4 183.1 206.1 183.1 224C183.1 227.4 186.2 230.5 189.5 231.6C192.7 232.7 196.3 231.6 198.4 228.8L198.4 228.8L198.6 228.5C198.8 228.3 198.1 228 199.3 227.6C199.1 226.8 200.9 225.7 202.1 224.3C204.6 221.4 208.1 217.7 212.3 213.1C221.1 206.2 231.2 200 239.1 200C248.8 200 258.9 206.2 267.7 213.1C271.9 217.7 275.4 221.4 277.9 224.3C279.1 225.7 280 226.8 280.7 227.6C281 228 281.2 228.3 281.4 228.5L281.6 228.8L281.6 228.8zM450.5 231.6C453.8 230.5 456 227.4 456 224C456 206.1 449.3 188.4 439.4 175.2C429.6 162.2 415.5 152 400 152C384.5 152 370.4 162.2 360.6 175.2C350.7 188.4 344 206.1 344 224C344 227.4 346.2 230.5 349.5 231.6C352.7 232.7 356.3 231.6 358.4 228.8L358.4 228.8L358.6 228.5C358.8 228.3 358.1 228 359.3 227.6C359.1 226.8 360.9 225.7 362.1 224.3C364.6 221.4 368.1 217.7 372.3 213.1C381.1 206.2 391.2 200 400 200C408.8 200 418.9 206.2 427.7 213.1C431.9 217.7 435.4 221.4 437.9 224.3C439.1 225.7 440 226.8 440.7 227.6C441 228 441.2 228.3 441.4 228.5L441.6 228.8L441.6 228.8C443.7 231.6 447.3 232.7 450.5 231.6V231.6zM68.69 299.3C62.44 293.1 62.44 282.9 68.69 276.7C74.93 270.4 85.06 270.4 91.31 276.7L170.3 355.7C175.4 360.8 184 357.2 184 350.1V330.4C184 319.4 192.1 310.4 204 310.4C215 310.4 224 319.4 224 330.4V416.8C224 469.4 181.4 512 128.8 512C103.6 512 79.34 501.1 61.49 484.1L4.686 427.3C-1.562 421.1-1.562 410.9 4.686 404.7C10.93 398.4 21.07 398.4 27.31 404.7L46.63 424C49.22 426.6 53.41 426.6 55.1 424C58.59 421.4 58.59 417.2 55.1 414.6L4.686 363.3C-1.562 357.1-1.562 346.9 4.686 340.7C10.93 334.4 21.07 334.4 27.31 340.7L78.63 392C81.22 394.6 85.41 394.6 87.1 392C90.59 389.4 90.59 385.2 87.1 382.6L20.69 315.3C14.44 309.1 14.44 298.9 20.69 292.7C26.93 286.4 37.06 286.4 43.31 292.7L110.6 360C113.2 362.6 117.4 362.6 119.1 360C122.6 357.4 122.6 353.2 119.1 350.6L68.69 299.3zM520 350.6C517.4 353.2 517.4 357.4 520 360C522.6 362.6 526.8 362.6 529.4 360L596.7 292.7C602.9 286.4 613.1 286.4 619.3 292.7C625.6 298.9 625.6 309.1 619.3 315.3L552 382.6C549.4 385.2 549.4 389.4 552 392C554.6 394.6 558.8 394.6 561.4 392L612.7 340.7C618.9 334.4 629.1 334.4 635.3 340.7C641.6 346.9 641.6 357.1 635.3 363.3L584 414.6C581.4 417.2 581.4 421.4 584 424C586.6 426.6 590.8 426.6 593.4 424L612.7 404.7C618.9 398.4 629.1 398.4 635.3 404.7C641.6 410.9 641.6 421.1 635.3 427.3L578.5 484.1C560.7 501.1 536.4 512 511.2 512C458.6 512 416 469.4 416 416.8V330.4C416 319.4 424.1 310.4 436 310.4C447 310.4 456 319.4 456 330.4V350.1C456 357.2 464.6 360.8 469.7 355.7L548.7 276.7C554.9 270.4 565.1 270.4 571.3 276.7C577.6 282.9 577.6 293.1 571.3 299.3L520 350.6z"></path></svg>
                  </span>
                  <span>Dataset</span>
                  </a> -->

                  <span class="link-block">
                    <a href="https://huggingface.co/datasets/aimagelab/ViSU-Text"
                       class="external-link button is-normal is-rounded is-dark">
                      <span class="icon">
                          <svg class="svg-inline--fa fa-face-smiling-hands" aria-hidden="true" focusable="false" data-prefix="fas" data-icon="face-smiling-hands" role="img" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 640 512" data-fa-i2svg=""><path fill="currentColor" d="M411.1 495.3C382.8 506.1 352.1 512 319.1 512C287.9 512 257.2 506.1 228.9 495.3C245.9 473.7 255.1 446.4 255.1 416.8V386C274.1 394.4 295.4 400 319.1 400C344.6 400 365.9 394.4 384 386V416.8C384 446.4 394.1 473.7 411.1 495.3V495.3zM575.7 242.6C558.8 236.8 539.5 240.6 526.1 254.1L478.1 301.1C469.6 287.4 453.9 278.4 436 278.4C407.3 278.4 384 301.7 384 330.4V349.6C367.2 360.3 345.9 368 319.1 368C294.1 368 272.8 360.3 255.1 349.6V330.4C255.1 301.7 232.7 278.4 203.1 278.4C186.1 278.4 170.4 287.4 161 301.1L113.9 254.1C100.5 240.6 81.15 236.8 64.34 242.6C71.31 107.5 183.1 0 319.1 0C456.9 0 568.7 107.5 575.7 242.6V242.6zM281.6 228.8C283.7 231.6 287.3 232.7 290.5 231.6C293.8 230.5 295.1 227.4 295.1 224C295.1 206.1 289.3 188.4 279.4 175.2C269.6 162.2 255.5 152 239.1 152C224.5 152 210.4 162.2 200.6 175.2C190.7 188.4 183.1 206.1 183.1 224C183.1 227.4 186.2 230.5 189.5 231.6C192.7 232.7 196.3 231.6 198.4 228.8L198.4 228.8L198.6 228.5C198.8 228.3 198.1 228 199.3 227.6C199.1 226.8 200.9 225.7 202.1 224.3C204.6 221.4 208.1 217.7 212.3 213.1C221.1 206.2 231.2 200 239.1 200C248.8 200 258.9 206.2 267.7 213.1C271.9 217.7 275.4 221.4 277.9 224.3C279.1 225.7 280 226.8 280.7 227.6C281 228 281.2 228.3 281.4 228.5L281.6 228.8L281.6 228.8zM450.5 231.6C453.8 230.5 456 227.4 456 224C456 206.1 449.3 188.4 439.4 175.2C429.6 162.2 415.5 152 400 152C384.5 152 370.4 162.2 360.6 175.2C350.7 188.4 344 206.1 344 224C344 227.4 346.2 230.5 349.5 231.6C352.7 232.7 356.3 231.6 358.4 228.8L358.4 228.8L358.6 228.5C358.8 228.3 358.1 228 359.3 227.6C359.1 226.8 360.9 225.7 362.1 224.3C364.6 221.4 368.1 217.7 372.3 213.1C381.1 206.2 391.2 200 400 200C408.8 200 418.9 206.2 427.7 213.1C431.9 217.7 435.4 221.4 437.9 224.3C439.1 225.7 440 226.8 440.7 227.6C441 228 441.2 228.3 441.4 228.5L441.6 228.8L441.6 228.8C443.7 231.6 447.3 232.7 450.5 231.6V231.6zM68.69 299.3C62.44 293.1 62.44 282.9 68.69 276.7C74.93 270.4 85.06 270.4 91.31 276.7L170.3 355.7C175.4 360.8 184 357.2 184 350.1V330.4C184 319.4 192.1 310.4 204 310.4C215 310.4 224 319.4 224 330.4V416.8C224 469.4 181.4 512 128.8 512C103.6 512 79.34 501.1 61.49 484.1L4.686 427.3C-1.562 421.1-1.562 410.9 4.686 404.7C10.93 398.4 21.07 398.4 27.31 404.7L46.63 424C49.22 426.6 53.41 426.6 55.1 424C58.59 421.4 58.59 417.2 55.1 414.6L4.686 363.3C-1.562 357.1-1.562 346.9 4.686 340.7C10.93 334.4 21.07 334.4 27.31 340.7L78.63 392C81.22 394.6 85.41 394.6 87.1 392C90.59 389.4 90.59 385.2 87.1 382.6L20.69 315.3C14.44 309.1 14.44 298.9 20.69 292.7C26.93 286.4 37.06 286.4 43.31 292.7L110.6 360C113.2 362.6 117.4 362.6 119.1 360C122.6 357.4 122.6 353.2 119.1 350.6L68.69 299.3zM520 350.6C517.4 353.2 517.4 357.4 520 360C522.6 362.6 526.8 362.6 529.4 360L596.7 292.7C602.9 286.4 613.1 286.4 619.3 292.7C625.6 298.9 625.6 309.1 619.3 315.3L552 382.6C549.4 385.2 549.4 389.4 552 392C554.6 394.6 558.8 394.6 561.4 392L612.7 340.7C618.9 334.4 629.1 334.4 635.3 340.7C641.6 346.9 641.6 357.1 635.3 363.3L584 414.6C581.4 417.2 581.4 421.4 584 424C586.6 426.6 590.8 426.6 593.4 424L612.7 404.7C618.9 398.4 629.1 398.4 635.3 404.7C641.6 410.9 641.6 421.1 635.3 427.3L578.5 484.1C560.7 501.1 536.4 512 511.2 512C458.6 512 416 469.4 416 416.8V330.4C416 319.4 424.1 310.4 436 310.4C447 310.4 456 319.4 456 330.4V350.1C456 357.2 464.6 360.8 469.7 355.7L548.7 276.7C554.9 270.4 565.1 270.4 571.3 276.7C577.6 282.9 577.6 293.1 571.3 299.3L520 350.6z"></path></svg>
                        </span>
                      <span>Dataset</span>
                    </a>
                  </span>

                  <span class="link-block">
                    <a href="https://huggingface.co/collections/aimagelab/safe-clip-668d0a0ca697b69d66433338"
                       class="external-link button is-normal is-rounded is-dark">
                      <span class="icon">
                          <svg class="svg-inline--fa fa-face-smiling-hands" aria-hidden="true" focusable="false" data-prefix="fas" data-icon="face-smiling-hands" role="img" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 640 512" data-fa-i2svg=""><path fill="currentColor" d="M411.1 495.3C382.8 506.1 352.1 512 319.1 512C287.9 512 257.2 506.1 228.9 495.3C245.9 473.7 255.1 446.4 255.1 416.8V386C274.1 394.4 295.4 400 319.1 400C344.6 400 365.9 394.4 384 386V416.8C384 446.4 394.1 473.7 411.1 495.3V495.3zM575.7 242.6C558.8 236.8 539.5 240.6 526.1 254.1L478.1 301.1C469.6 287.4 453.9 278.4 436 278.4C407.3 278.4 384 301.7 384 330.4V349.6C367.2 360.3 345.9 368 319.1 368C294.1 368 272.8 360.3 255.1 349.6V330.4C255.1 301.7 232.7 278.4 203.1 278.4C186.1 278.4 170.4 287.4 161 301.1L113.9 254.1C100.5 240.6 81.15 236.8 64.34 242.6C71.31 107.5 183.1 0 319.1 0C456.9 0 568.7 107.5 575.7 242.6V242.6zM281.6 228.8C283.7 231.6 287.3 232.7 290.5 231.6C293.8 230.5 295.1 227.4 295.1 224C295.1 206.1 289.3 188.4 279.4 175.2C269.6 162.2 255.5 152 239.1 152C224.5 152 210.4 162.2 200.6 175.2C190.7 188.4 183.1 206.1 183.1 224C183.1 227.4 186.2 230.5 189.5 231.6C192.7 232.7 196.3 231.6 198.4 228.8L198.4 228.8L198.6 228.5C198.8 228.3 198.1 228 199.3 227.6C199.1 226.8 200.9 225.7 202.1 224.3C204.6 221.4 208.1 217.7 212.3 213.1C221.1 206.2 231.2 200 239.1 200C248.8 200 258.9 206.2 267.7 213.1C271.9 217.7 275.4 221.4 277.9 224.3C279.1 225.7 280 226.8 280.7 227.6C281 228 281.2 228.3 281.4 228.5L281.6 228.8L281.6 228.8zM450.5 231.6C453.8 230.5 456 227.4 456 224C456 206.1 449.3 188.4 439.4 175.2C429.6 162.2 415.5 152 400 152C384.5 152 370.4 162.2 360.6 175.2C350.7 188.4 344 206.1 344 224C344 227.4 346.2 230.5 349.5 231.6C352.7 232.7 356.3 231.6 358.4 228.8L358.4 228.8L358.6 228.5C358.8 228.3 358.1 228 359.3 227.6C359.1 226.8 360.9 225.7 362.1 224.3C364.6 221.4 368.1 217.7 372.3 213.1C381.1 206.2 391.2 200 400 200C408.8 200 418.9 206.2 427.7 213.1C431.9 217.7 435.4 221.4 437.9 224.3C439.1 225.7 440 226.8 440.7 227.6C441 228 441.2 228.3 441.4 228.5L441.6 228.8L441.6 228.8C443.7 231.6 447.3 232.7 450.5 231.6V231.6zM68.69 299.3C62.44 293.1 62.44 282.9 68.69 276.7C74.93 270.4 85.06 270.4 91.31 276.7L170.3 355.7C175.4 360.8 184 357.2 184 350.1V330.4C184 319.4 192.1 310.4 204 310.4C215 310.4 224 319.4 224 330.4V416.8C224 469.4 181.4 512 128.8 512C103.6 512 79.34 501.1 61.49 484.1L4.686 427.3C-1.562 421.1-1.562 410.9 4.686 404.7C10.93 398.4 21.07 398.4 27.31 404.7L46.63 424C49.22 426.6 53.41 426.6 55.1 424C58.59 421.4 58.59 417.2 55.1 414.6L4.686 363.3C-1.562 357.1-1.562 346.9 4.686 340.7C10.93 334.4 21.07 334.4 27.31 340.7L78.63 392C81.22 394.6 85.41 394.6 87.1 392C90.59 389.4 90.59 385.2 87.1 382.6L20.69 315.3C14.44 309.1 14.44 298.9 20.69 292.7C26.93 286.4 37.06 286.4 43.31 292.7L110.6 360C113.2 362.6 117.4 362.6 119.1 360C122.6 357.4 122.6 353.2 119.1 350.6L68.69 299.3zM520 350.6C517.4 353.2 517.4 357.4 520 360C522.6 362.6 526.8 362.6 529.4 360L596.7 292.7C602.9 286.4 613.1 286.4 619.3 292.7C625.6 298.9 625.6 309.1 619.3 315.3L552 382.6C549.4 385.2 549.4 389.4 552 392C554.6 394.6 558.8 394.6 561.4 392L612.7 340.7C618.9 334.4 629.1 334.4 635.3 340.7C641.6 346.9 641.6 357.1 635.3 363.3L584 414.6C581.4 417.2 581.4 421.4 584 424C586.6 426.6 590.8 426.6 593.4 424L612.7 404.7C618.9 398.4 629.1 398.4 635.3 404.7C641.6 410.9 641.6 421.1 635.3 427.3L578.5 484.1C560.7 501.1 536.4 512 511.2 512C458.6 512 416 469.4 416 416.8V330.4C416 319.4 424.1 310.4 436 310.4C447 310.4 456 319.4 456 330.4V350.1C456 357.2 464.6 360.8 469.7 355.7L548.7 276.7C554.9 270.4 565.1 270.4 571.3 276.7C577.6 282.9 577.6 293.1 571.3 299.3L520 350.6z"></path></svg>
                      </span>
                      <span>Models</span>
                      </a>

            </div>

          </div>
        </div>
      </div>
    </div>
  </div>
</section>
<section class="hero teaser">
  <div class="container is-max-desktop">
    <div class="hero-body">
      <!--<img src="./static/images/huggingface_d3_gif.gif">-->
      <h2 class="subtitle has-text-centered">
        Safe-CLIP is an ehnanced vision-and-language model designed to mitigate the risks associated with NSFW (Not Safe For Work) 
        content in AI applications.

        Based on the CLIP model, Safe-CLIP is fine-tuned to serve the association between linguistic and visual concepts, 
        ensuring safer outputs in text-to-image and image-to-text retrieval and generation tasks.

        <br> <br>
        <i><b>Warning:</b> This project involves explicit sexual content, racially insensitive language, 
        and other material that may be harmful or disturbing to certain users. 
        Please use this content solely for research purposes and proceed with caution. </i>
        
      </h2>
    </div>
  </div>
</section>


<section class="hero is-light is-small">
  <div class="hero-body">
    <div class="container">
      <div id="results-carousel" class="carousel results-carousel">
        <div class="item item-steve">
          <img src="static/images/Picture2.png" id="steve" height="100%">
        </div>
        <div class="item item-chair-tp">
          <img src="static/images/Picture3.png" id="chair-tp" height="100%">
        </div>
        <div class="item item-shiba">
          <img src="static/images/Picture1.png" id="shiba" height="100%">
        </div>
        <div class="item item-fullbody">
          <img src="static/images/Picture4.png" id="fullbody" height="100%">
        </div>
        <div class="item item-blueshirt">
          <img src="static/images/Picture5.png" id="blueshirt" height="100%">
        </div>
        <div class="item item-mask">
          <img src="static/images/Picture6.png" id="mask" height="100%">
        </div>
      </div>
      <!-- Use existing buttons -->
      <button class="slider-navigation-previous" id="prev-button">&#10094;</button>
      <button class="slider-navigation-next" id="next-button">&#10095;</button>
    </div>
  </div>
</section>


<section class="section">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Abstract</h2>
        <div class="content has-text-justified">
          <p>
            Large-scale vision-and-language models, such as CLIP, are
            typically trained on web-scale data, which can introduce inappropriate content and lead to the development of unsafe and biased behavior. 
            This, in turn, hampers their applicability in sensitive and trustworthy contexts and could raise significant concerns in their adoption.
          </p>
          <p>
            Our research introduces a novel approach to enhancing the safety of
            vision-and-language models by diminishing their sensitivity to NSFW
            (not safe for work) inputs. In particular, our methodology seeks to sever
            “toxic” linguistic and visual concepts, unlearning the linkage between
            unsafe linguistic or visual items and unsafe regions of the embedding
            space. We show how this can be done by fine-tuning a CLIP model on
            synthetic data obtained from a large language model trained to convert between safe and unsafe sentences, and a text-to-image generator.
          </p>
          <p>
            We conduct extensive experiments on the resulting embedding space for
            cross-modal retrieval, text-to-image, and image-to-text generation, where
            we show that our model can be remarkably employed with pre-trained
            generative models. 
          </p>
        </div>
      </div>
    </div>
  </div>
</section>

  <section class="hero is-light is-small">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Model architecture</h2>
        <!-- <h2 class="title is-5">Safe-CLIP Removing NSFW Concepts</h2> -->
        <div class="content has-text-centered">
          <img src="static/images/model.jpg" class="center" width="760">
        </div>
      </div>
    </div> 
  </section>

<section class="section">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <!-- <h2 class="title is-3">Model description</h2> -->
        <div class="content has-text-justified">
          <p>
            Safe-CLIP is a vision-and-language model designed to mitigate the risks associated with NSFW (Not Safe For Work) content in AI applications.
            To achieved this, we fine-tuned the CLIP model on a synthetic dataset obtained from a large language model trained to convert between safe and unsafe sentences, 
            and a text-to-image generator.

            The fine-tuning process is based on a mixture of cosine distance and contrastive losses to modifiy the CLIP model's embedding space,
            but at the same time preserving the original CLIP's ability to associate linguistic and visual concepts.
        </div>
      </div>
    </div>
  </div>
</section>

<section class="hero is-light is-small">
  <div class="columns is-centered has-text-centered">
    <div class="column is-four-fifths">
      <h2 class="title is-3">Safe-CLIP Tasks</h2>
      <!-- <h2 class="title is-5">Safe-CLIP Removing NSFW Concepts</h2> -->
      <div class="content has-text-centered">
        <img src="static/images/first_page.jpg" class="center" width="760">
      </div>
    </div>
  </div> 
</section>

<section class="section">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <!-- <h2 class="title is-3">Qualitative Results / Tasks</h2> -->
        <div class="content has-text-justified">
          <p>
            We evaluate Safe-CLIP on a variety of tasks. Specifically, cross-modal retrieval consider both text and image as queries information 
            to retrieve the most relevant information from the other modality.

            Moreover, we evaluate the model on text-to-image and image-to-text generation tasks.
            Employing the stable diffusion pipeline for the former and LLaVA family models for the latter.
            
            It is important to mention that thanks to its development Safe-CLIP can be effectively and simply integrated in pre-trained generative models.
            
      </div>
    </div>
  </div>
</section>

<section class="hero is-light is-small">
  <div class="columns is-centered has-text-centered">
    <div class="column is-four-fifths">
      <h2 class="title is-3">Qualitative Results</h2>
      <!-- <h2 class="title is-5">Safe-CLIP Removing NSFW Concepts</h2> -->
      <div class="content has-text-centered">
        <img src="static/images/unimore_logo.png" class="center" width="760">
      </div>
    </div>
  </div> 
</section>



<section class="section">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Citation</h2>
        <div class="content has-text-justified">
          <pre><code>@inproceedings{poppi2024removing,
            title={{Safe-CLIP: Removing NSFW Concepts from Vision-and-Language Models}},
            author={Poppi, Samuele and Poppi, Tobia and Cocchi, Federico and Cornia, Marcella and Baraldi, Lorenzo and Cucchiara, Rita},
            booktitle={Proceedings of the European Conference on Computer Vision},
            year={2024}
          }</code></pre>
        </div>
      </div>
    </div>
  </div>
</section>

<!-- <section class="section">
    <div class="container is-max-desktop">
      <div class="container is-max-desktop content">
        <h2 class="title">Citation</h2>
        <pre><code>@article{poppi2024removing,
          title={{Safe-CLIP: Removing NSFW Concepts from Vision-and-Language Models}},
          author={Poppi, Samuele and Poppi, Tobia and Cocchi, Federico and Cornia, Marcella and Baraldi, Lorenzo and Cucchiara, Rita},
          journal={arXiv preprint arXiv:2311.16254},
          year={2024}
        }</code></pre>
      </div>
    </div>
  </section> -->


<footer class="footer">
  <div class="container">
    <!-- <div class="content has-text-centered footer-icons">
      <a class=""
         href="https://international.unimore.it/">
        <img  src="https://aimagelab.ing.unimore.it/imagelab/uploadedImages/001097.png">
      </a>
      <a class="" href="https://www.leonardo.com/it/innovation-technology/leonardo-labs" class="external-link" disabled>
        <img  src="https://aimagelab.ing.unimore.it/imagelab/uploadedImages/001098.png">
      </a>
      <a class="" href="https://www.elsa-ai.eu/" class="external-link" disabled>
        <img  src="https://aimagelab.ing.unimore.it/imagelab/uploadedImages/001099.png">
      </a>
    </div> -->


    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content">
          <p>
            This website is licensed under a <a rel="license"
                                                href="http://creativecommons.org/licenses/by-sa/4.0/">Creative
            Commons Attribution-ShareAlike 4.0 International License</a>.
          </p>
          <p>
            Website powered by AImageLab |  HTML template from <a href="https://github.com/nerfies/nerfies.github.io">Nerfies</a>
          </p>
        </div>
      </div>
    </div>
  </div>
</footer>

<script>
  document.addEventListener('DOMContentLoaded', function () {
    const carousels = bulmaCarousel.attach('#results-carousel', {
      slidesToScroll: 1,
      slidesToShow: 5,
      infinite: true,
    });

    carousels.forEach(carousel => {
      document.getElementById('prev-button').addEventListener('click', function () {
        carousel.previous();
      });
      

      document.getElementById('next-button').addEventListener('click', function () {
        carousel.next();
      });

      function autoPlayCarousel() {
        setInterval(() => {
          carousel.next();
        }, 3000); // Change image every 3 seconds
      }

      autoPlayCarousel();
    });
  });
</script>

<!-- Include the Bulma Carousel JS file -->
<script src="https://cdnjs.cloudflare.com/ajax/libs/bulma-carousel/4.0.3/js/bulma-carousel.min.js"></script>


</body>
</html>